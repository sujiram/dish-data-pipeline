
## üßæ Expected Deliverables Overview

| Deliverable | Description |
|--------------|-------------|
| **`data_pipeline.py`** | Main extraction, transformation, and loading (ETL) script. It fetches paginated API data, performs cleaning, applies data quality checks, and loads into BigQuery. |
| **`requirements.txt`** | List of Python dependencies required to execute the pipeline locally or via Docker/Airflow. |
| **`README.md`** | Project documentation containing setup, execution, and validation instructions. |
| **Sample Output Files** | Example API extraction results saved in GCS (`raw_api_data/...`) and sample output screenshots attached |
| **Populated BigQuery Tables / CSVs** | Final target tables: `tgt_daily_visits`, `tgt_ga_sessions`, and audit log `load_audit`. Equivalent CSVs provided if BigQuery access is restricted. |

---

## üöÄ Project Overview

This project demonstrates an end-to-end data engineering solution built for **DISH Deutschland‚Äôs Data Engineer Technical Assessment**.

The pipeline:
- Extracts data from **two API endpoints** (`daily-visits`, `ga-sessions-data`).
- Saves raw API responses into **Google Cloud Storage (GCS)** partitioned by date.
- Cleans and flattens JSON data.
- Runs **data quality (DQ) checks** to validate schema, nulls, duplicates, and record counts.
- Loads data into **BigQuery staging tables**.
- Uses **MERGE statements** to upsert into final `tgt_` tables.
- Logs all job statuses and file references into an **audit table**.
- Supports orchestration using **Apache Airflow** and deployment via **Docker**.

---

## ‚öôÔ∏è Setup Instructions

### 1Ô∏è‚É£ Clone Repository
```bash
git clone https://github.com/<your-username>/dish-data-pipeline.git
cd dish-data-pipeline
```

### 2Ô∏è‚É£ Create Virtual Environment & Install Dependencies
```bash
python -m venv .venv
source .venv/bin/activate       # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Configure Environment
Copy the configuration template and update it with your details:
```bash
cp pipeline/config_file_template.py pipeline/config_file.py
```

Edit `config_file.py` with your actual project values:
```python
PROJECT_ID = "your-gcp-project-id"
BUCKET_NAME = "your-gcs-bucket"
DATASET = "dish_dataset"
BASE_URL = "https://dish-second-course-gateway-2tximoqc.nw.gateway.dev"
HEADERS = {"Content-Type": "application/json"}
ENDPOINTS = {
    "daily_visits": "daily-visits",
    "ga_sessions": "ga-sessions-data"
}
```

---

## ‚ñ∂Ô∏è Execution Options

### üß© Option 1 ‚Äî Run Locally
```bash
python pipeline/data_pipeline.py
```

### üê≥ Option 2 ‚Äî Run with Docker
```bash
docker build -t dish-data-pipeline .
docker run --rm   -v $(pwd)/pipeline/config_file.py:/app/pipeline/config_file.py   dish-data-pipeline
```

### ‚òÅÔ∏è Option 3 ‚Äî Airflow Orchestration
Use the DAG file `dags/etl_google_analytics_dag.py`.

**Schedule:** Runs at 6:00 AM and 6:00 PM on **Wednesdays**  
**Retries:** 2 attempts  
**Delay:** 5 minutes between retries  
**Timeout:** 3 minutes per task  

---

## üß™ Data Flow Summary

### üì• Extract
- API calls to:
  - `/daily-visits`
  - `/ga-sessions-data`
- Each page of API data is stored in:
  ```
  gs://<your-bucket>/raw_api_data/<endpoint>/year=YYYY/month=MM/day=DD/
  ```

### üîß Transform
- Nested JSON flattened using `pandas.json_normalize()`
- Added metadata:
  - `load_timestamp` (UTC)
  - `source_file`
- Data quality checks:
  - Missing column validation
  - Null detection
  - Duplicate detection
  - Low record count alerts
- Deduplication:
  - `daily_visits`: `(visit_date, source_file)`
  - `ga_sessions`: `(visitId, source_file)`

### üì§ Load
- Data loaded to **BigQuery staging tables**:
  ```
  dish_dataset.staging_daily_visits
  dish_dataset.staging_ga_sessions
  ```
- Incrementally merged to final **target tables**:
  ```
  dish_dataset.tgt_daily_visits
  dish_dataset.tgt_ga_sessions
  ```
- Load audit logged in:
  ```
  dish_dataset.load_audit
  ```

---

## üßÆ Sample Output 
sample_output/ load_audit.csv

<img width="622" height="235" alt="image" src="https://github.com/user-attachments/assets/2081edd5-55f1-4ca2-8c55-5fd040757140" />
<img width="626" height="307" alt="image" src="https://github.com/user-attachments/assets/88edc80c-096a-4e49-967d-ba3e9f1b99b2" />
<img width="602" height="236" alt="image" src="https://github.com/user-attachments/assets/6b1c8ac9-59ae-486b-a507-d2dc7adf88cd" />
<img width="612" height="242" alt="image" src="https://github.com/user-attachments/assets/c89bda1b-464b-4c0c-becf-1ec8e6eb2145" />
<img width="635" height="238" alt="image" src="https://github.com/user-attachments/assets/8a844994-c38a-4b1b-be78-caca5cd59427" />
<img width="623" height="242" alt="image" src="https://github.com/user-attachments/assets/d20fbfbe-8f52-4cca-9406-0f08cdd5c784" />

Example ‚Äî `load_audit.csv`
| table_name     | record_count | status   | load_timestamp       | source_files |
|----------------|---------------|----------|----------------------|---------------|
| daily_visits   | 367           | SUCCESS  | 2025-10-22 18:30:00Z | raw_api_data/daily_visits/... |
| ga_sessions    | 2509          | SUCCESS  | 2025-10-22 18:32:00Z | raw_api_data/ga_sessions/... |

---

## üß† Data Quality & Governance

### Data Quality Monitoring
| Check Type | Description | Action |
|-------------|-------------|--------|
| Missing Columns | Verifies expected schema | Skips or logs as failure |
| Nulls in Key Fields | Detects missing keys | Fails and logs |
| Duplicates | Deduplicates on primary keys | Removes duplicates and continues |
| Record Count | Alerts on low volume (<5 rows) | Logs warning |

### Data Governance
- **Lineage:** API ‚Üí GCS ‚Üí BigQuery (staging ‚Üí target)
- **Metadata:** `load_timestamp`, `source_file`, `record_count`, `status`
- **Audit Trail:** Each run logged in `load_audit`
- **Data Privacy:** No PII stored; config file excluded from GitHub
- **Documentation:** DAG details in `dags/DAG.md`

---

## üß© Folder Structure

```
dish-data-pipeline/
‚îÇ
‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ config_file_template.py
‚îÇ   ‚îî‚îÄ‚îÄ config_file.py           # (local only; excluded from GitHub)
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ etl_google_analytics_dag.py
‚îÇ   ‚îî‚îÄ‚îÄ DAG.md
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ sample_output.zip
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

---

## üìä BigQuery Tables Overview

| Layer | Table | Purpose |
|-------|--------|----------|
| **Raw (GCS)** | raw_api_data/... | Raw JSON from APIs |
| **Staging** | `staging_daily_visits`, `staging_ga_sessions` | Temporary clean data |
| **Target** | `tgt_daily_visits`, `tgt_ga_sessions` | Final merged production tables |
| **Audit** | `load_audit` | Tracks load metadata and DQ status |

---

## üìò Contact

**Author:** Sumathi R  
**Role:** Data Engineer Candidate  
**Location:** Germany  
**Tools Used:** Python ¬∑ BigQuery ¬∑ GCS ¬∑ Airflow ¬∑ Docker  
